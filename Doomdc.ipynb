{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-08 21:28:37.424717: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-08 21:28:37.462311: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-08 21:28:37.475573: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-08 21:28:37.544647: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-08 21:28:38.519498: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from vizdoom import *\n",
    "import os\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from stable_baselines3.common import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = DoomGame()\n",
    "game.load_config(os.path.join(scenarios_path, \"defend_the_center.cfg\"))\n",
    "game.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = np.identity(3, dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = game.get_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 240, 320)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.screen_buffer.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 26., 100.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.game_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: -1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: -1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: 3.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: -1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 1.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: 0.0\n",
      "reward: -1.0\n",
      "Result: -1.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 10 \n",
    "for episode in range(episodes): \n",
    "    # Create a new episode or game \n",
    "    game.new_episode()\n",
    "    # Check the game isn't done \n",
    "    while not game.is_episode_finished(): \n",
    "        # Get the game state \n",
    "        state = game.get_state()\n",
    "        # Get the game image \n",
    "        img = state.screen_buffer\n",
    "        # Get the game variables - ammo\n",
    "        info = state.game_variables\n",
    "        # Take an action\n",
    "        reward = game.make_action(random.choice(actions),4)\n",
    "        # Print rewward \n",
    "        print('reward:', reward) \n",
    "        time.sleep(0.2)\n",
    "    print('Result:', game.get_total_reward())\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## introducing the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VizDoomGym(Env):\n",
    "    def __init__(self, render=False):\n",
    "        super().__init__()\n",
    "\n",
    "        self.game = DoomGame()\n",
    "        self.game.load_config(os.path.join(scenarios_path, \"defend_the_center.cfg\"))\n",
    "\n",
    "        if render == False:\n",
    "            self.game.set_window_visible(False)\n",
    "        else:\n",
    "            self.game.set_window_visible(True)\n",
    "\n",
    "        self.game.init()\n",
    "\n",
    "        self.observation_space = Box(low=0, high =255, shape=(100,160,1), dtype=np.uint8)\n",
    "        self.action_space = Discrete(3)\n",
    "\n",
    "    def step(self, action):\n",
    "        actions = np.identity(3, dtype=np.uint8)\n",
    "        reward = self.game.make_action(actions[action], 4)\n",
    "\n",
    "        if self.game.get_state():\n",
    "            state = self.game.get_state().screen_buffer\n",
    "            state = self.grayscale(state)\n",
    "            info = self.game.get_state().game_variables\n",
    "        else:\n",
    "            state = np.zeros(self.observation_space.shape)\n",
    "            info = 0\n",
    "\n",
    "        info = {\"info\":info}\n",
    "        done = self.game.is_episode_finished()\n",
    "        \n",
    "        return state, reward, done, done, info\n",
    "    \n",
    "    def render():\n",
    "        pass\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        self.game.new_episode()\n",
    "        state = self.game.get_state().screen_buffer\n",
    "        info = self.game.get_state().game_variables\n",
    "        info = {\"info\":info}\n",
    "        return self.grayscale(state), info\n",
    "\n",
    "    def grayscale(self, observation):\n",
    "        gray = cv2.cvtColor(np.moveaxis(observation, 0, -1), cv2.COLOR_BGR2GRAY)\n",
    "        resize = cv2.resize(gray, (160, 100), interpolation=cv2.INTER_CUBIC)\n",
    "        state = np.reshape(resize, (100,160,1))\n",
    "        return state\n",
    "\n",
    "    def close(self):\n",
    "        self.game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[ 5],\n",
       "         [10],\n",
       "         [ 7],\n",
       "         ...,\n",
       "         [ 4],\n",
       "         [ 6],\n",
       "         [10]],\n",
       " \n",
       "        [[ 3],\n",
       "         [ 3],\n",
       "         [ 4],\n",
       "         ...,\n",
       "         [ 5],\n",
       "         [ 3],\n",
       "         [ 7]],\n",
       " \n",
       "        [[11],\n",
       "         [ 7],\n",
       "         [10],\n",
       "         ...,\n",
       "         [ 9],\n",
       "         [ 6],\n",
       "         [ 6]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[75],\n",
       "         [63],\n",
       "         [62],\n",
       "         ...,\n",
       "         [44],\n",
       "         [71],\n",
       "         [60]],\n",
       " \n",
       "        [[15],\n",
       "         [48],\n",
       "         [47],\n",
       "         ...,\n",
       "         [49],\n",
       "         [69],\n",
       "         [47]],\n",
       " \n",
       "        [[22],\n",
       "         [14],\n",
       "         [26],\n",
       "         ...,\n",
       "         [57],\n",
       "         [37],\n",
       "         [39]]], dtype=uint8),\n",
       " {'info': array([ 26., 100.])})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndLoggingCallback(BaseCallback):\n",
    "\n",
    "    def __init__(self, check_freq, save_path, verbose=1):\n",
    "        super(TrainAndLoggingCallback, self).__init__(verbose)\n",
    "        self.check_freq = check_freq\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def _init_callback(self):\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "    \n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            model_path = os.path.join(self.save_path, 'best_model_{}'.format(self.n_calls))\n",
    "            self.model.save(model_path)\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKPOINT_DIR = './train/train_defend_it2'\n",
    "LOG_DIR = './logs/log_defend_it2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = TrainAndLoggingCallback(check_freq=10000, save_path=CHECKPOINT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Wrapping the env in a VecTransposeImage.\n"
     ]
    }
   ],
   "source": [
    "model = PPO('CnnPolicy', env, tensorboard_log=LOG_DIR, verbose=1, learning_rate=0.0001, n_steps=4096)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./logs/log_defend_it2/PPO_1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 81.7     |\n",
      "|    ep_rew_mean     | 0.286    |\n",
      "| time/              |          |\n",
      "|    fps             | 410      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 84.9         |\n",
      "|    ep_rew_mean          | 0.458        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 307          |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 26           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0055123577 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.00764      |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0373       |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00331     |\n",
      "|    value_loss           | 0.108        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 92.9         |\n",
      "|    ep_rew_mean          | 1.02         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 285          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 12288        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097611565 |\n",
      "|    clip_fraction        | 0.11         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.381        |\n",
      "|    learning_rate        | 0.0001       |\n",
      "|    loss                 | 0.0152       |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.0183      |\n",
      "|    value_loss           | 0.0868       |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 101         |\n",
      "|    ep_rew_mean          | 1.76        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 267         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 61          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012992978 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0115     |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0297     |\n",
      "|    value_loss           | 0.112       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 109         |\n",
      "|    ep_rew_mean          | 2.54        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 253         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013986008 |\n",
      "|    clip_fraction        | 0.203       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.539       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.036      |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0352     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 118         |\n",
      "|    ep_rew_mean          | 3.45        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 241         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017739449 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.971      |\n",
      "|    explained_variance   | 0.605       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0124     |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0417     |\n",
      "|    value_loss           | 0.151       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 125         |\n",
      "|    ep_rew_mean          | 4.08        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 234         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 122         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021816423 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.947      |\n",
      "|    explained_variance   | 0.611       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0441     |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0455     |\n",
      "|    value_loss           | 0.169       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 126        |\n",
      "|    ep_rew_mean          | 4.28       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 228        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 143        |\n",
      "|    total_timesteps      | 32768      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01927767 |\n",
      "|    clip_fraction        | 0.232      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.91      |\n",
      "|    explained_variance   | 0.765      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.00804   |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0382    |\n",
      "|    value_loss           | 0.153      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 131         |\n",
      "|    ep_rew_mean          | 4.78        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 224         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020515226 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.883      |\n",
      "|    explained_variance   | 0.798       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0357     |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0418     |\n",
      "|    value_loss           | 0.138       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 134        |\n",
      "|    ep_rew_mean          | 5.21       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 220        |\n",
      "|    iterations           | 10         |\n",
      "|    time_elapsed         | 185        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02140075 |\n",
      "|    clip_fraction        | 0.234      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.861     |\n",
      "|    explained_variance   | 0.794      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0507    |\n",
      "|    n_updates            | 90         |\n",
      "|    policy_gradient_loss | -0.0425    |\n",
      "|    value_loss           | 0.157      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 139         |\n",
      "|    ep_rew_mean          | 5.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 217         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 207         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023849519 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.82       |\n",
      "|    explained_variance   | 0.827       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0447     |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0424     |\n",
      "|    value_loss           | 0.166       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 148        |\n",
      "|    ep_rew_mean          | 6.28       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 228        |\n",
      "|    total_timesteps      | 49152      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02493907 |\n",
      "|    clip_fraction        | 0.263      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.822     |\n",
      "|    explained_variance   | 0.848      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | 0.0201     |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0411    |\n",
      "|    value_loss           | 0.164      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 152         |\n",
      "|    ep_rew_mean          | 6.67        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 249         |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025852438 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.811      |\n",
      "|    explained_variance   | 0.858       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0245     |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0413     |\n",
      "|    value_loss           | 0.162       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 159         |\n",
      "|    ep_rew_mean          | 7.19        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023916764 |\n",
      "|    clip_fraction        | 0.233       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.786      |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.00318    |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0399     |\n",
      "|    value_loss           | 0.164       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 165        |\n",
      "|    ep_rew_mean          | 7.69       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 15         |\n",
      "|    time_elapsed         | 292        |\n",
      "|    total_timesteps      | 61440      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02640156 |\n",
      "|    clip_fraction        | 0.244      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.763     |\n",
      "|    explained_variance   | 0.884      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0285    |\n",
      "|    n_updates            | 140        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.173      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 168         |\n",
      "|    ep_rew_mean          | 8.01        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 209         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027248567 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.726      |\n",
      "|    explained_variance   | 0.895       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0469     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0408     |\n",
      "|    value_loss           | 0.144       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 175         |\n",
      "|    ep_rew_mean          | 8.61        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 334         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031405944 |\n",
      "|    clip_fraction        | 0.232       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.691      |\n",
      "|    explained_variance   | 0.904       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0233     |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0404     |\n",
      "|    value_loss           | 0.159       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 185         |\n",
      "|    ep_rew_mean          | 9.28        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 206         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 356         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030559096 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.664      |\n",
      "|    explained_variance   | 0.922       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.031      |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.0451     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 192         |\n",
      "|    ep_rew_mean          | 9.94        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 205         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 379         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031214269 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.624      |\n",
      "|    explained_variance   | 0.926       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0375     |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0414     |\n",
      "|    value_loss           | 0.135       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 200         |\n",
      "|    ep_rew_mean          | 10.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 401         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031682625 |\n",
      "|    clip_fraction        | 0.222       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.581      |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0297     |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0416     |\n",
      "|    value_loss           | 0.127       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 213         |\n",
      "|    ep_rew_mean          | 11.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 203         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 422         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029635414 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.556      |\n",
      "|    explained_variance   | 0.94        |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0148     |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0377     |\n",
      "|    value_loss           | 0.126       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 219         |\n",
      "|    ep_rew_mean          | 12          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 202         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031505425 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.535      |\n",
      "|    explained_variance   | 0.954       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0381     |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0419     |\n",
      "|    value_loss           | 0.102       |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 227         |\n",
      "|    ep_rew_mean          | 12.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 200         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 469         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030998705 |\n",
      "|    clip_fraction        | 0.205       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.518      |\n",
      "|    explained_variance   | 0.935       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | 0.0266      |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0358     |\n",
      "|    value_loss           | 0.142       |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 232        |\n",
      "|    ep_rew_mean          | 13         |\n",
      "| time/                   |            |\n",
      "|    fps                  | 199        |\n",
      "|    iterations           | 24         |\n",
      "|    time_elapsed         | 492        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02857903 |\n",
      "|    clip_fraction        | 0.21       |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.558     |\n",
      "|    explained_variance   | 0.945      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0318    |\n",
      "|    n_updates            | 230        |\n",
      "|    policy_gradient_loss | -0.0441    |\n",
      "|    value_loss           | 0.121      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 236         |\n",
      "|    ep_rew_mean          | 13.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 198         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 515         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032381497 |\n",
      "|    clip_fraction        | 0.215       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.537      |\n",
      "|    explained_variance   | 0.949       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0474     |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.043      |\n",
      "|    value_loss           | 0.116       |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 241       |\n",
      "|    ep_rew_mean          | 13.8      |\n",
      "| time/                   |           |\n",
      "|    fps                  | 197       |\n",
      "|    iterations           | 26        |\n",
      "|    time_elapsed         | 538       |\n",
      "|    total_timesteps      | 106496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0297334 |\n",
      "|    clip_fraction        | 0.203     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -0.527    |\n",
      "|    explained_variance   | 0.955     |\n",
      "|    learning_rate        | 0.0001    |\n",
      "|    loss                 | -0.0348   |\n",
      "|    n_updates            | 250       |\n",
      "|    policy_gradient_loss | -0.0385   |\n",
      "|    value_loss           | 0.119     |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 236        |\n",
      "|    ep_rew_mean          | 13.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 197        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 560        |\n",
      "|    total_timesteps      | 110592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02875149 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.557     |\n",
      "|    explained_variance   | 0.956      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0298    |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0395    |\n",
      "|    value_loss           | 0.13       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 234        |\n",
      "|    ep_rew_mean          | 13.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 196        |\n",
      "|    iterations           | 28         |\n",
      "|    time_elapsed         | 583        |\n",
      "|    total_timesteps      | 114688     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03515193 |\n",
      "|    clip_fraction        | 0.237      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.561     |\n",
      "|    explained_variance   | 0.958      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0302    |\n",
      "|    n_updates            | 270        |\n",
      "|    policy_gradient_loss | -0.0444    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 227        |\n",
      "|    ep_rew_mean          | 13.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 194        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 611        |\n",
      "|    total_timesteps      | 118784     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03730803 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.537     |\n",
      "|    explained_variance   | 0.96       |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0356    |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.043     |\n",
      "|    value_loss           | 0.106      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 220         |\n",
      "|    ep_rew_mean          | 12.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 635         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.037748463 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.534      |\n",
      "|    explained_variance   | 0.945       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0576     |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0462     |\n",
      "|    value_loss           | 0.14        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 218        |\n",
      "|    ep_rew_mean          | 12.7       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 193        |\n",
      "|    iterations           | 31         |\n",
      "|    time_elapsed         | 655        |\n",
      "|    total_timesteps      | 126976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03287761 |\n",
      "|    clip_fraction        | 0.239      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.516     |\n",
      "|    explained_variance   | 0.941      |\n",
      "|    learning_rate        | 0.0001     |\n",
      "|    loss                 | -0.0233    |\n",
      "|    n_updates            | 300        |\n",
      "|    policy_gradient_loss | -0.0418    |\n",
      "|    value_loss           | 0.111      |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 216         |\n",
      "|    ep_rew_mean          | 12.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 193         |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 676         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034283936 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.524      |\n",
      "|    explained_variance   | 0.956       |\n",
      "|    learning_rate        | 0.0001      |\n",
      "|    loss                 | -0.0363     |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.0454     |\n",
      "|    value_loss           | 0.115       |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[1;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:179\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# Convert to pytorch tensor or to TensorDict\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     obs_tensor \u001b[38;5;241m=\u001b[39m obs_as_tensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 179\u001b[0m     actions, values, log_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m actions \u001b[38;5;241m=\u001b[39m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;66;03m# Rescale and perform action\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py:654\u001b[0m, in \u001b[0;36mActorCriticPolicy.forward\u001b[0;34m(self, obs, deterministic)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;66;03m# Evaluate the values for the given observations\u001b[39;00m\n\u001b[1;32m    653\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(latent_vf)\n\u001b[0;32m--> 654\u001b[0m distribution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_action_dist_from_latent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlatent_pi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m actions \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mget_actions(deterministic\u001b[38;5;241m=\u001b[39mdeterministic)\n\u001b[1;32m    656\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m distribution\u001b[38;5;241m.\u001b[39mlog_prob(actions)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/policies.py:697\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(mean_actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_std)\n\u001b[1;32m    695\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[1;32m    696\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[0;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction_dist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproba_distribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist, MultiCategoricalDistribution):\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# Here mean_actions are the flattened logits\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dist\u001b[38;5;241m.\u001b[39mproba_distribution(action_logits\u001b[38;5;241m=\u001b[39mmean_actions)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/stable_baselines3/common/distributions.py:288\u001b[0m, in \u001b[0;36mCategoricalDistribution.proba_distribution\u001b[0;34m(self, action_logits)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mproba_distribution\u001b[39m(\u001b[38;5;28mself\u001b[39m: SelfCategoricalDistribution, action_logits: th\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfCategoricalDistribution:\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution \u001b[38;5;241m=\u001b[39m \u001b[43mCategorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction_logits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/distributions/categorical.py:65\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;66;03m# Normalize\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits \u001b[38;5;241m=\u001b[39m logits \u001b[38;5;241m-\u001b[39m logits\u001b[38;5;241m.\u001b[39mlogsumexp(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprobs \u001b[38;5;28;01mif\u001b[39;00m probs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_events \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m batch_shape \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_param\u001b[38;5;241m.\u001b[39mndimension() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mSize()\n\u001b[1;32m     69\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=600000, callback=callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('./train/train_defend_it2/best_model_120000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = VizDoomGym(render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.7"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward for episode 13.0 is 0\n",
      "Total Reward for episode 5.0 is 1\n",
      "Total Reward for episode 14.0 is 2\n",
      "Total Reward for episode 15.0 is 3\n"
     ]
    }
   ],
   "source": [
    "for episode in range(4): \n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    while not done: \n",
    "        action, _ = model.predict(obs)\n",
    "        obs, reward, done, _, info = env.step(action)\n",
    "        time.sleep(0.10)\n",
    "        total_reward += reward\n",
    "    print('Total Reward for episode {} is {}'.format(total_reward, episode))\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
